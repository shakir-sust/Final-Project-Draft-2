---
title: "Data Science Applied to Ag - Final Project - ML"
format:
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Md Shakir Moazzem, Umar Munir
---

# Start of data wrangling

# Introduction  

This script contains the data wrangling steps for the final project.  

# Setup  

##Loading packages  

The following code chunk will load necessary packages.

```{r Setup, message=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("readxl")
#install.packages("janitor")
#install.packages("dplyr")
#install.packages("tidyr")
#install.packages("readr")
#install.packages("lubridata")
#install.packages("stringr")

# Loading packages 

library(tidyverse)
library(readxl) # to read excel files
library(janitor) # to clean data; helps fix and standardize the column names
library(dplyr) # wrangling
library(tidyr) # wrangling
library(readr) # to export csv
library(lubridate)
library(stringr)

```


## Reading data  

The following code chunk will read the csv files for the 3 training data sets

```{r training data import, message=F, warning=F}

#reading the csv files for the 3 training data sets 

trait_df <- read_csv("../data/training/training_trait.csv") 
meta_df  <- read_csv("../data/training/training_meta.csv")
soil_df  <- read_csv("../data/training/training_soil.csv")

```

## EDA

```{r}

summary(trait_df)

View(trait_df)

```
```{r}

summary(meta_df)

```



```{r}

summary(soil_df)

```


# Data wrangling on "trait_df"

The code below creates a function to adjust the yield_mg_ha for 15.5% grain moisture.

```{r}

# Function to transform yield to 15.5% moisture
adjust_yield <- function(yield_mg_ha, grain_moisture) {
  yield_mg_ha * (100 - grain_moisture) / (100 - 15.5)
}

```


The code below conducts data wrangling on "trait_df"

```{r clean_and_summarize_trait, message=F, warning=F}



trait_clean <- trait_df %>%
  select(-block) %>%
  mutate(
    site = str_remove_all(site, "[a-z]"), # removing all lowercase letters from site
    site = str_replace(site, "-.*$", ""), # keeping only text before any dash
    site = str_replace(site, "_.*$", "") # keeping only text before any underscore
  ) %>%
  group_by(year, site, hybrid) %>%
  summarize(
    yield_mg_ha    = mean(yield_mg_ha, na.rm = TRUE),
    grain_moisture = mean(grain_moisture, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    adjusted_yield = adjust_yield(yield_mg_ha, grain_moisture),
  ) %>%
  select(-yield_mg_ha, -grain_moisture) %>%
  ungroup()


trait_clean

```



```{r exporting wrangled data set}

#write_csv(trait_avg_df,
          #"../data/training_trait_clean.csv")
```


# Data wrangling on "soil_df"

The following code chunk will conduct data wrangling on  "training_soil.csv"


```{r data_wrangling_training_soil, message=F, warning=F}

soil_clean <- soil_df %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  ungroup()

soil_clean
```


```{r exporting wrangled data set}

#write_csv(soil_clean,
          #"../data/training_soil_clean.csv")
```


# Data wrangling on "meta_df"

The following code chunk will conduct data wrangling on  "training_meta.csv".

```{r clean_meta_sites}

meta_clean <- meta_df %>%
  rename(
    lon = longitude,
    lat = latitude
  ) %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  distinct(year, site, .keep_all = TRUE) %>%
  select(-previous_crop) %>%
  clean_names() %>%
  ungroup() 
  

meta_clean

```



# EDA for cleaned data


```{r}
summary(trait_clean)
```


```{r}
summary(soil_clean)
```


```{r}
summary(meta_clean)
```


```{r exporting wrangled data set}

#write_csv(meta_clean,
          #"../data/training_meta_clean.csv")
```


# Merging all 3 cleaned data frames


```{r}

merged_clean  <- trait_clean %>%
  left_join(soil_clean, by = c("year", "site")) %>%
  left_join(meta_clean, by = c("year", "site"))

merged_clean
```

```{r}
summary(merged_clean)
```


```{r exporting merged data set}

#write_csv(merged_clean,
          #"../data/training_merged_clean.csv")
```

# End of Data Wrangling 

#Start of Open Source Daymet weather data download

The following code chunk will load necessary packages.  

```{r Setup, message=F, warning=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("sf") #to manipulate vector geospatial files
#install.packages("daymetr") #to retrieve data from daymet database website through R
#install.packages("remotes") #to install R packages that are not available on CRAN, and available on GitHub
#remotes::install_github("ropensci/USAboundaries") 
#remotes::install_github("ropensci/USAboundariesData")

# Loading packages

library(tidyverse) #need to load "tidyverse" package at first
library(sf) # for US map #to manipulate vector geo-spatial points
library(daymetr) #to retrieve data from daymet database website through R
library(remotes)
library(USAboundaries) # for US state boundaries
library(USAboundariesData)

```


The following code chunk will create a map of the USA and plot the sites in the map based on their latitude and longitude.


```{r create map of USA and add points, message=F, warning=F}

states <- us_states() %>% 
  filter( !(state_abbr %in% c("PR", "AK", "HI")) ) #to remove "PR" (Puerto Rico), "AK" (Alaska), and "HI" (Hawaii)
  
ggplot() +
  geom_sf(data = states) + #"geom_sf()" is used to plot "sf" object, which we just created above as "states" object; plots all states and territories of USA
  geom_point(data = merged_clean,
             aes(x = lon, #"Longitude" goes on longitude
                 y = lat) #"Latitude" goes on latitude
             ) +
  labs(
    title = "Corn Trial Site Locations (2014–2023)",
    x     = "Longitude",
    y     = "Latitude"
  )

```

The following code chunk will keep the observations for site-years having latitude and longitude withing the Daymet range of co-ordinates.

Declaration of AI use: the following code chunk was inspired and subsequently modified on the basis of code initially generated by ChatGPT


```{r}

# Define Daymet bounding box (WGS-84)
min_lat <- 14.53
max_lat <- 52.00
min_lon <- -131.104
max_lon <- -52.95

# Filter merged to Daymet’s valid range, dropping any NA coords
merged_daymet <- merged_clean %>%
  filter(
    !is.na(lat),
    !is.na(lon),
    lat  >= min_lat,
    lat  <= max_lat,
    lon  >= min_lon,
    lon  <= max_lon
  )

# Report how many rows remain (and were dropped)
message("Rows kept: ", nrow(merged_daymet), 
        " (dropped: ", nrow(merged_clean) - nrow(merged_daymet), ")")


```
The following code chunk will extract unique combinations of year, site, and their coordinates.

```{r unique_site_years_with_coords}

site_year_df <- merged_daymet %>%
  select(year, site, lon, lat) %>%  # need to include longitude and latitude along with site-years
  distinct() %>%                    
  arrange(year, site)

site_year_df

```

The following code chunk will download the weather data for all unique combinations of year, site, and their coordinates in the "site_year_df" object.

```{r}

weather_daymet_all <- site_year_df %>% 
  mutate(weather = pmap(list(.y = year, 
                             .site = site, 
                             .lat = lat, 
                             .lon = lon), 
                        function(.y, .site, .lat, .lon) 
                          download_daymet( 
                            site = .site, 
                            lat = .lat, #specifying ".lat" placeholder for "lat = " argument
                            lon = .lon, #specifying ".lon" placeholder for "lon = " argument
                            start = .y, 
                            end = .y, 
                            simplify = T,
                            silent = T) %>% #end of " download_daymet()" function
                          rename(.year = year,
                                 .site = site) 
                        )) 


weather_daymet_all

```


The following code chunk will unnest the weather column. 

```{r}

weather_daymet_unnest <- weather_daymet_all %>%
  select(year, site, weather) %>% 
  unnest(weather) %>% 
  pivot_wider(names_from = measurement, 
              values_from = value) %>% 
  janitor::clean_names()

weather_daymet_unnest

View(weather_daymet_unnest)

```


# Merging the weather data retrieved from Daymet to the "merged_clean" data

The following code chunk will merge the weather data retrieved from Daymet to the "merged_clean" data

```{r merge_weather_with_data}

# Join daily weather onto your cleaned trial data by year & site

daymet_all_unnest <- merged_clean %>%
  left_join(
    weather_daymet_unnest,
    by = c("year", "site")
  )

fieldweather <- daymet_all_unnest

fieldweather

View(fieldweather)

```


## Exporting the merged data


```{r}

#write_csv(daymet_all_unnest,
          #"../data/merged_fieldweatherdata.csv"
          #)

```


# End of retreiving weather data from Daymet 

# Start of feature engineering

## Setup 

```{r}

#install.packages("ggridges")

library(ggridges)
library(tidyverse)

```



```{r}

#fieldweather <- read_csv("../data/merged_fieldweatherdata.csv")

#fieldweather

```

The following code chunk will keep desired variables that we will use further and get abbreviated month name based on the date.

```{r}

fe_month <- fieldweather %>%
  # Selecting needed variables
  dplyr::select(year, 
                site, 
                hybrid, 
                lon, 
                lat, 
                yday, 
                yield = adjusted_yield, 
                soil.ph = soilpH, 
                soil.om.pct = om_pct, 
                soil.k.ppm = soilk_ppm, 
                soil.p.ppm = soilp_ppm, 
                dayl.s = dayl_s, #to rename variable name from "dayl_s" to dayl.s
                prcp.mm = prcp_mm_day, #to rename variable name to "prcp.mm"
                srad.wm2 = srad_w_m_2,#to rename variable name to "srad.wm2"
                tmax.c = tmax_deg_c, #to rename variable name to "tmax.c"
                tmin.c = tmin_deg_c,#to rename variable name to "tmin.c"
                vp.pa = vp_pa #to rename variable name to "vp.pa"
                ) %>%
  # Creating a date class variable  
  mutate(date_chr = paste0(year, "/", yday)) %>% 
  mutate(date = as.Date(date_chr, "%Y/%j")) %>% 
  # Extracting month from date  
  mutate(month = month(date)) %>% 
  mutate(month_abb = month(date, label = T)) #To get abbreviated month name e.g., Jan, Feb, Mar,...,Dec #month_abb is in "ord" (ordinal) format

fe_month
```



The following code chunk will summarize daily weather variables based on month.  

```{r fe_month_sum}

fe_month_sum <- fe_month %>%
  group_by(year, site, hybrid, month_abb, yield) %>% #If we do a summarise() after group_by(), any column that's not in the group_by() is gone, so we need to include "yield" in the group_by() to include it in the data frame because "yield" is our response variable so we must keep it in the data frame 
  #Because we are gonna be applying a "summarize()" function to different columns, we are gonna use a function called "across()" 
  summarise(across(.cols = c(soil.ph, 
                             soil.om.pct, 
                             soil.k.ppm, 
                             soil.p.ppm,
                             dayl.s,
                             srad.wm2,
                             tmax.c,
                             tmin.c,
                             vp.pa),
                   .fns = mean, #do not indicate the actual function "mean()", just use the word "mean"
                   .names = "mean_{.col}"), #specifying the weather variables that we want their mean as new column variables #1st across() is applying the "mean" function (to summarize "mean")
            across(.cols = prcp.mm,
                   .fns = sum,
                   .names = "sum_{.col}"
                   ) #specifying the weather variable (prcp.mm) that we want its sum as new column variable #2nd across() is summarizing sum #2nd across() is applying the "sum" function (to summarize "sum")
            ) %>%
  ungroup() #To convert from "group" to "tibble"


fe_month_sum

```


The following code chunk will check "tmax.c" and "prcp.mm" for the first site-year and month for the purpose of double checking to make sure that we did everything okay the way we intended. 

```{r}

fe_month %>%
  filter(year == 2014 & 
           site == "DEH1" &
           hybrid == "B37/MO17" &
           month_abb == "Jan") %>%
  summarise(tmax.c = mean(tmax.c),
            prcp.mm = sum(prcp.mm))


```

The code below will put the month as part of the column name. 


```{r fe_month_sum_wide}

fe_month_sum_wide <- fe_month_sum %>%
  pivot_longer(cols = mean_soil.ph:sum_prcp.mm) %>% 
  mutate(varname = paste0(name, "_", month_abb)) %>% 
  dplyr::select(-name, -month_abb) %>% 
  pivot_wider(names_from = varname,
              values_from = value) %>%
  # Rounding to one decimal point
  mutate(across(c(4:ncol(.)), ~round(., 1) )) %>%
  select( -mean_soil.ph_NA, -mean_soil.om.pct_NA, -mean_soil.k.ppm_NA, -mean_soil.p.ppm_NA, -mean_dayl.s_NA, -mean_srad.wm2_NA, -mean_tmax.c_NA, -mean_tmin.c_NA, -mean_vp.pa_NA, -sum_prcp.mm_NA)

fe_month_sum_wide  

```


# EDA round 2  
Let's make a ridge plot to visualize the distribution of one variable over months.  

```{r, message=FALSE, warning=FALSE}

#install.packages("ggridges")
library(ggridges) #really powerful package to plot distributions e.g., density plot

ggplot(data = fe_month_sum,
       aes(x = mean_tmax.c,
           y = month_abb,
           fill = stat(x) #fill = stat(x): specific to "ggridges"
           )
       ) +
  geom_density_ridges_gradient(scale = 3,
                               rel_min_height = 0.01) + #"the 2nd argument is"rel_min_height = 0.01": to cut down the tails of the distribution to look nice
  scale_fill_viridis_c(option = "C") + #"viridis"scale_fill_viridis_c()": color-blind-friendly color scale #argument (option = "C") to change the color to "magma" because magma is weather related; "option = " varies from A to F
  theme(legend.position = "none") #"theme(legend.position = "none")": to remove the legend from the graph

```

Now let's do that for all weather variables [ we will automate using map2() to conduct iteration ]

```{r}

finalplots <- fe_month_sum %>%
  pivot_longer(cols = mean_soil.ph:sum_prcp.mm) %>% #we need to do pivot_longer() to iterate over the weather variables
  group_by(name) %>%
  nest() %>% #for iteration, we use "group_by()" followed by "nest()" in combo
  #we will use map2() since we need to iterate over 2 columns
  mutate(plot = map2(data, name, #map2() takes 2 arguments: the 1st argument becomes .x, the 2nd argument becomes .y #we must use map2() with a mutate() at first for iteration #"map2(data, name" : we want to iterate over "name" for "data"
                     ~ ggplot( data = .x, # .x represent "data" in the map2() function, so we need to use data = .x [= "data" from map2() ] to feed the "data" from map2() function as the data of ggplot() #.x is the iterating column of map2() that is a place holder for the 1st argument of map2()
       aes(x = value,
           y = month_abb,
           fill = stat(x)
           )
       ) +
  geom_density_ridges_gradient(scale = 3,
                               rel_min_height = 0.01) + 
  scale_fill_viridis_c(option = "C") + 
  theme(legend.position = "none") +
  labs(x = .y) #to rename the x-axis as the variable name # .y is the placeholder for "name" (i.e., variable name) in map2() function
                     )) 
  
finalplots

```




```{r}

finalplots$plot #$plot to print all the ggplots for each variables

```


```{r}

write_csv(fe_month_sum_wide,
           "../data/weather_monthsum.csv")

```


# End of feature engineering

# Start of XGboost

# ML Workflow with XGBoost

## Step 1: Load Libraries

We begin by loading the required libraries:

```{r}
#| message: false
#| warning: false
#install.packages("xgboost") #new pacakage
#install.packages("caret")

library(tidymodels)   # Core framework for modeling (includes recipes, workflows, parsnip, etc.)
library(finetune)     # Additional tuning strategies (e.g., racing, ANOVA-based tuning)
library(vip)          # For plotting variable importance from fitted models
library(xgboost)      # XGBoost implementation in R
library(ranger)       # Fast implementation of Random Forests
library(tidyverse)    # Data wrangling and visualization
library(doParallel)   # For parallel computing (useful during resampling/tuning)
library(caret)       # Other great library for Machine Learning 
```



# Load the data set


```{r weather}

#weather <- read_csv("../data/weather_monthsum.csv")

#weather

```

The following code chunk will rename the "fe_month_sum_wide" data frame as "weather".   

```{r}

weather <- fe_month_sum_wide
weather

```

# ML workflow

## 1. Pre-processing

```{r weather_split}

set.seed(931735) # Setting seed to get reproducible results 

weather_split <- initial_split(
  weather, 
  prop = .7, # proption of split same as previous codes
  strata = yield  # Stratify by target variable
  )

weather_split

```

### a. Data split

For data split, let's use **70% training / 30% testing**.

```{r weather_train}

weather_train <- training(weather_split)  # 70% of data

weather_train #This is your traing data frame

```


```{r weather_test}

weather_test <- testing(weather_split)    # 30% of data

weather_test

```


### b. Distribution of target variable

```{r distribution}
ggplot() +
  geom_density(data = weather_train, 
               aes(x = yield),
               color = "red") +
  geom_density(data = weather_test, 
               aes(x = yield),
               color = "blue") 
  
```

### c. Data processing with recipe


```{r weather_recipe}

# Create recipe for data preprocessing
weather_recipe <- recipe(yield ~ ., data = weather_train) %>% 
  # Remove identifier columns and months not in growing season
  step_rm(
    year,       # Remove year identifier
    site,       # Remove site identifier
    hybrid,     # Remove site identifier
    matches("Jan|Feb|Mar|Dec")  # Remove non-growing season months
  ) 


weather_recipe

```


```{r weather_prep}
# Prep the recipe to estimate any required statistics
weather_prep <- weather_recipe %>% 
  prep()

# Examine preprocessing steps
weather_prep
```

## 2. Training

### a. Model specification

First, let's specify:\
- the **type of model** we want to train\
- which **engine** we want to use\
- which **mode** we want to use

> Elastic nets can only be run for a numerical response variable. XgBoost can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

XgBoost **hyperparameters**:

-   **`trees`**: The number of boosting rounds (i.e., how many trees will be added sequentially).\
-   **`tree_depth`**: Controls how deep each individual tree can grow. Deeper trees can capture more complex interactions but also increase the risk of overfitting.\
-   **`min_n`**: Minimum number of observations required in a node for it to be split. Acts as a regularization tool to prevent overly specific splits.\
-   **`learn_rate`**: Also known as `eta`, it controls how much each additional tree contributes to the overall model. Smaller values make the model more stable but require more trees.

```{r xgb_spec}

xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),  # Maximum depth of each tree
  min_n = tune(),  # Minimum samples required to split a node
  learn_rate = tune()
  ) %>% #Specifying XgBoost as our model type, asking to tune the hyperparameters
  set_engine("xgboost") %>% #specify engine 
  set_mode("regression")  # Set to mode
      
xgb_spec

```

### b. Cross-validation setup

We use 5-fold cross-validation to evaluate model performance during tuning:

```{r}

set.seed(235) #34549

resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv

resampling_foldcv$splits[[1]]

```

### c. Hyperparameter grid with Latin Hypercube Sampling

We use Latin hypercube sampling to generate a diverse grid of hyperparameter combinations:

```{r }

xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  min_n(),
  learn_rate(),
  size = 20
)

xgb_grid

```

The following code chunk will plot the hyperparameter combinations.

```{r}
ggplot(data = xgb_grid,
       aes(x = tree_depth, 
           y = min_n)) +
  geom_point(aes(color = factor(learn_rate), #coloring the bubbles based on learn_rate
                 size = trees), #size of the bubbles are based on the tress
             alpha = .5,
             show.legend = FALSE)
```

## 3. Model Tuning

The following code chunk will conduct model tuning.

```{r xgb_grid_result}

#install.packages("doParallel")
#install.packages("parallel")

library(doParallel)
library(parallel)

set.seed(76544)

#parallel processing
#registerDoParallel(cores = parallel::detectCores()-1) #starts parallel processing

xgb_res <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

#stopImplicitCluster() #ends parallel processing

beepr::beep()

xgb_res$.metrics[[2]]

```


## 4. Select Best Models

We select the best models using three strategies (lowest RMSE, highest R2, within 1 SE, within 2% loss).

The following code chunk will select best model based on lowest RMSE.

```{r}

# Based on lowest RMSE
best_rmse <- xgb_res %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse")

best_rmse

```

The following code chunk will select best model based on lowest RMSE within 1% loss


```{r}
# Based on lowest RMSE within 1% loss
best_rmse_pct_loss <- xgb_res %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 1
                     )%>% 
  mutate(source = "best_rmse_pct_loss")

best_rmse_pct_loss
```

The following code chunk will select best model based on lowest RMSE within 1 SE.


```{r}
# Based on lowest RMSE within 1 se
best_rmse_one_std_err <- xgb_res %>% 
  select_by_one_std_err(metric = "rmse",
                        eval_time = 100,
                        trees
                        )%>% 
  mutate(source = "best_rmse_one_std_err")

best_rmse_one_std_err
```


The following code chunk will select best model based on greatest R2.


```{r}
# Based on greatest R2
best_r2 <- xgb_res %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2")

best_r2
```

The following code chunk will select best model based on greatest R2 within 1% loss.


```{r}
# Based on greatest R2 within 1% loss
best_r2_pct_loss <- xgb_res %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 1
                     ) %>% 
  mutate(source = "best_r2_pct_loss")

best_r2_pct_loss
```


The following code chunk will select best model based on greatest R2 within 1 SE


```{r}
# Based on greatest R2 within 1 se
best_r2_one_std_error <- xgb_res %>% 
  select_by_one_std_err(metric = "rsq",
                        eval_time = 100,
                        trees
                        ) %>%
  mutate(source = "best_r2_one_std_error")

best_r2_one_std_error
```

## Compare and Finalize Model

The following code chunk will compare all models

```{r comparing values}
best_rmse %>% 
  bind_rows(best_rmse_pct_loss, 
            best_rmse_one_std_err, 
            best_r2, 
            best_r2_pct_loss, 
            best_r2_one_std_error)
```


## 5. Final Specification

The following code chunk will conduct final specification.

```{r final_spec_fit}
final_spec <- boost_tree(
  trees = best_r2$trees,           # Number of boosting rounds (trees)
  tree_depth = best_r2$tree_depth, # Maximum depth of each tree
  min_n = best_r2$min_n,           # Minimum number of samples to split a node
  learn_rate = best_r2$learn_rate  # Learning rate (step size shrinkage)
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

final_spec
```



## 6. Final Fit and Predictions

## Validation

The following code chunk will conduct validation.

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```


## 7. Evaluate on *Test Set*

The following code chunk will evaluate fit metrics on the test set.

```{r final_fit_metrics}
final_fit %>%
  collect_metrics()
```

 

## 8. Evaluate on Training Set

The following code chunk will evaluate fit metrics on the training set.

```{r}
final_spec %>%
  fit(yield ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(yield, .pred) %>%
  bind_rows(
    
    
# R2
final_spec %>%
  fit(yield ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rsq(yield, .pred))
```


## 9. Predicted vs Observed Plot

The following code chunk will create a Predicted vs Observed Plot.

```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = yield,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous() +
  scale_y_continuous() 
```


## 10. Variable Importance

The following code chunk will create a Variable Importance plot.

```{r final_spec}
final_spec %>%
  fit(yield ~ .,
         data = bake(weather_prep, weather_train)) %>% #There little change in variable improtance if you use full dataset
    vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```

# Obtaining "yield" prediction on the "testing_submission.csv" data set

```{r}

# Reading & cleaning testing data 

submission   <- read_csv("../data/testing/testing_submission.csv") %>% 
  mutate(site = str_remove_all(site, "[a-z]"),
         site = str_replace(site, "-.*$", ""),
         site = str_replace(site, "_.*$", ""))

meta_test    <- read_csv("../data/testing/testing_meta.csv")  %>% 
  rename(lon = longitude, lat = latitude) %>% 
  mutate(site = str_remove_all(site, "[a-z]"),
         site = str_replace(site, "-.*$", ""),
         site = str_replace(site, "_.*$", "")) %>% 
  distinct(year, site, .keep_all = TRUE) %>% 
  select(-previous_crop)

soil_test    <- read_csv("../data/testing/testing_soil.csv")  %>% 
  mutate(site = str_remove_all(site, "[a-z]"),
         site = str_replace(site, "-.*$", ""),
         site = str_replace(site, "_.*$", ""))

test_base <- submission %>%
  left_join(soil_test, by = c("year","site")) %>%
  left_join(meta_test, by = c("year","site"))

# Downloading Daymet weather for 2024 site‐years 
site_year_test <- test_base %>% 
  select(year, site, lon, lat) %>% 
  distinct()

weather_test <- site_year_test %>%
  mutate(weather = pmap(
    list(.y = year, .site = site, .lat = lat, .lon = lon),
    ~ download_daymet(site = .site,
                      lat   = .lat,
                      lon   = .lon,
                      start = .y,
                      end   = .y,
                      simplify = TRUE,
                      silent = TRUE)
  )) %>%
  select(year, site, weather) %>%
  unnest(weather) %>%
  pivot_wider(names_from = measurement, values_from = value) %>%
  #clean_names()

test_full <- test_base %>%
  left_join(weather_test, by = c("year","site"))

# Feature engineering monthly summaries for 2024 site‐years
fe_test <- test_full %>%
  # creating date & month
  mutate(date = as.Date(paste0(year, "/", yday), "%Y/%j"),
         month_abb = month(date, label = TRUE)) %>%
  # selecting and renaming to match training
  transmute(
    year, site, hybrid,
    month_abb,
    soil.ph       = soilp_h,    # will have to adjust these names if the clean_names() differ
    soil.om.pct   = om_pct,
    soil.k.ppm    = soilk_ppm,
    soil.p.ppm    = soilp_ppm,
    dayl.s        = dayl_s,
    prcp.mm       = prcp_mm_day,
    srad.wm2      = srad_w_m_2,
    tmax.c        = tmax_deg_c,
    tmin.c        = tmin_deg_c,
    vp.pa         = vp_pa
  ) %>%
  group_by(year, site, hybrid, month_abb) %>%
  summarise(
    mean_soil.ph     = mean(soil.ph,   na.rm = TRUE),
    mean_soil.om.pct = mean(soil.om.pct, na.rm = TRUE),
    mean_soil.k.ppm  = mean(soil.k.ppm,  na.rm = TRUE),
    mean_soil.p.ppm  = mean(soil.p.ppm,  na.rm = TRUE),
    mean_dayl.s      = mean(dayl.s,      na.rm = TRUE),
    sum_prcp.mm      = sum(prcp.mm,      na.rm = TRUE),
    mean_srad.wm2    = mean(srad.wm2,    na.rm = TRUE),
    mean_tmax.c      = mean(tmax.c,      na.rm = TRUE),
    mean_tmin.c      = mean(tmin.c,      na.rm = TRUE),
    mean_vp.pa       = mean(vp.pa,       na.omit = TRUE)
  ) %>%
  ungroup() %>%
  pivot_longer(
    cols = starts_with(c("mean_", "sum_")),
    names_to = "name",
    values_to = "value"
  ) %>%
  mutate(varname = paste0(name, "_", month_abb)) %>%
  select(-name, -month_abb) %>%
  pivot_wider(names_from = varname, values_from = value) %>%
  select(-ends_with("_NA"))

# Prepping test features & prediction 
test_prepped  <- bake(weather_prep, new_data = fe_test)

# fitting final model on all training data
final_model   <- final_spec %>% 
  fit(yield ~ ., data = bake(weather_prep, weather))

# generating predictions
predictions   <- predict(final_model, new_data = test_prepped)

# binding back 
submission_pred <- fe_test %>% 
  select(year, site, hybrid) %>% 
  bind_cols(predictions)

write_csv(submission_pred,
          "../data/testing/testing_submission_pred.csv")


```


