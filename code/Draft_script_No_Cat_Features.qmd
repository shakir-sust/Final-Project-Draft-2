---
title: "Data Science Applied to Ag - Final Project - ML"
format:
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Md Shakir Moazzem, Umar Munir
---

# Start of data wrangling

# Introduction  

This script contains the data wrangling steps for the final project.  

# Setup  

##Loading packages  

The following code chunk will load necessary packages.

```{r Setup, message=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("readxl")
#install.packages("janitor")
#install.packages("dplyr")
#install.packages("tidyr")
#install.packages("readr")
#install.packages("lubridata")
#install.packages("stringr")

# Loading packages 

library(tidyverse)
library(readxl) # to read excel files
library(janitor) # to clean data; helps fix and standardize the column names
library(dplyr) # wrangling
library(tidyr) # wrangling
library(readr) # to export csv
library(lubridate)
library(stringr)

```


## Reading data  

The following code chunk will read the csv files for the 3 training data sets

```{r training data import, message=F, warning=F}

#reading the csv files for the 3 training data sets 

trait_df <- read_csv("../data/training/training_trait.csv") 
meta_df  <- read_csv("../data/training/training_meta.csv")
soil_df  <- read_csv("../data/training/training_soil.csv")

```

## EDA

```{r}

summary(trait_df)

View(trait_df)

```
```{r}

summary(meta_df)

```



```{r}

summary(soil_df)

```


# Data wrangling on "trait_df"

The code below creates a function to adjust the yield_mg_ha for 15.5% grain moisture.

```{r}

# Function to transform yield to 15.5% moisture
adjust_yield <- function(yield_mg_ha, grain_moisture) {
  yield_mg_ha * (100 - grain_moisture) / (100 - 15.5)
}

```


The code below conducts data wrangling on "trait_df"

```{r clean_and_summarize_trait, message=F, warning=F}



trait_clean <- trait_df %>%
  select(-block) %>%
  mutate(
    site = str_remove_all(site, "[a-z]"), # removing all lowercase letters from site
    site = str_replace(site, "-.*$", ""), # keeping only text before any dash
    site = str_replace(site, "_.*$", "") # keeping only text before any underscore
  ) %>%
  group_by(year, site, hybrid) %>%
  summarize(
    yield_mg_ha    = mean(yield_mg_ha, na.rm = TRUE),
    grain_moisture = mean(grain_moisture, na.rm = TRUE),
    date_planted   = first(date_planted),
    date_harvested = first(date_harvested),
    .groups = "drop"
  ) %>%
  mutate(
    adjusted_yield = adjust_yield(yield_mg_ha, grain_moisture),
    planting_date  = as.Date(date_planted,   "%m/%j/%y"),
    harvest_date   = as.Date(date_harvested, "%m/%j/%y"),
    plant_doy      = yday(planting_date),
    harvest_doy    = yday(harvest_date),
  ) %>%
  select(-yield_mg_ha, -grain_moisture, -date_planted, -date_harvested, -planting_date, -harvest_date) %>%
  ungroup()


trait_clean

```



```{r exporting wrangled data set}

#write_csv(trait_avg_df,
          #"../data/training_trait_clean.csv")
```


# Data wrangling on "soil_df"

The following code chunk will conduct data wrangling on  "training_soil.csv"


```{r data_wrangling_training_soil, message=F, warning=F}

soil_clean <- soil_df %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  ungroup()

soil_clean
```


```{r exporting wrangled data set}

#write_csv(soil_clean,
          #"../data/training_soil_clean.csv")
```


# Data wrangling on "meta_df"

The following code chunk will conduct data wrangling on  "training_meta.csv".

```{r clean_meta_sites}

meta_df2 <- meta_df %>%
  rename(
    lon = longitude,
    lat = latitude
  ) %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  distinct(year, site, .keep_all = TRUE) %>%
  ungroup()

meta_df2

```


The code below will create categories for previous crops.


```{r}

meta_clean <- meta_df2 %>%
  mutate(
    # 1) lowercase & trim
    prev_crop_raw = str_to_lower(str_squish(previous_crop)),
    # 2) recode into your desired levels
    prev_crop = case_when(
      prev_crop_raw == "cotton"                                   ~ "cotton",
      prev_crop_raw == "peanut"                                   ~ "peanut",
      prev_crop_raw %in% c("soybean", "soybeans")                 ~ "soybean",
      prev_crop_raw == "sorghum"                                  ~ "sorghum",
      prev_crop_raw %in% c("wheat", "winter wheat", "2019/20 wheat") ~ "wheat",
      TRUE                                                         ~ "others"
    ) %>%
    # 3) make it a factor with the exact ordering you want
    factor(levels = c("cotton", "peanut", "soybean", "sorghum", "wheat", "others"))
  ) %>%
  select(-previous_crop, -prev_crop_raw) %>%
  clean_names()

meta_clean

```


# EDA for cleaned data


```{r}
summary(trait_clean)
```
```{r}
summary(soil_clean)
```


```{r}
summary(meta_clean)
```


```{r exporting wrangled data set}

#write_csv(meta_clean,
          #"../data/training_meta_clean.csv")
```


# Merging all 3 cleaned data frames


```{r}

merged_clean  <- trait_clean %>%
  left_join(soil_clean, by = c("year", "site")) %>%
  left_join(meta_clean, by = c("year", "site"))

merged_clean
```

```{r}
summary(merged_clean)
```


```{r exporting merged data set}

#write_csv(merged_clean,
          #"../data/training_merged_clean.csv")
```

# End of Data Wrangling 

#Start of Open Source Daymet weather data download

The following code chunk will load necessary packages.  

```{r Setup, message=F, warning=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("sf") #to manipulate vector geospatial files
#install.packages("daymetr") #to retrieve data from daymet database website through R
#install.packages("remotes") #to install R packages that are not available on CRAN, and available on GitHub
#remotes::install_github("ropensci/USAboundaries") 
#remotes::install_github("ropensci/USAboundariesData")

# Loading packages

library(tidyverse) #need to load "tidyverse" package at first
library(sf) # for US map #to manipulate vector geo-spatial points
library(daymetr) #to retrieve data from daymet database website through R
library(remotes)
library(USAboundaries) # for US state boundaries
library(USAboundariesData)

```


The following code chunk will create a map of the USA and plot the sites in the map based on their latitude and longitude.


```{r create map of USA and add points, message=F, warning=F}

states <- us_states() %>% 
  filter( !(state_abbr %in% c("PR", "AK", "HI")) ) #to remove "PR" (Puerto Rico), "AK" (Alaska), and "HI" (Hawaii)
  
ggplot() +
  geom_sf(data = states) + #"geom_sf()" is used to plot "sf" object, which we just created above as "states" object; plots all states and territories of USA
  geom_point(data = merged_clean,
             aes(x = lon, #"Longitude" goes on longitude
                 y = lat) #"Latitude" goes on latitude
             ) +
  labs(
    title = "Corn Trial Site Locations (2014–2023)",
    x     = "Longitude",
    y     = "Latitude"
  )

```

The following code chunk will keep the observations for site-years having latitude and longitude withing the Daymet range of co-ordinates.

Declaration of AI use: the following code chunk was inspired and subsequently modified on the basis of code initially generated by ChatGPT


```{r}

# Define Daymet bounding box (WGS-84)
min_lat <- 14.53
max_lat <- 52.00
min_lon <- -131.104
max_lon <- -52.95

# Filter merged to Daymet’s valid range, dropping any NA coords
merged_daymet <- merged_clean %>%
  filter(
    !is.na(lat),
    !is.na(lon),
    lat  >= min_lat,
    lat  <= max_lat,
    lon  >= min_lon,
    lon  <= max_lon
  )

# Report how many rows remain (and were dropped)
message("Rows kept: ", nrow(merged_daymet), 
        " (dropped: ", nrow(merged_clean) - nrow(merged_daymet), ")")


```
The following code chunk will extract unique combinations of year, site, and their coordinates.

```{r unique_site_years_with_coords}

site_year_df <- merged_daymet %>%
  select(year, site, lon, lat) %>%  # need to include longitude and latitude along with site-years
  distinct() %>%                    
  arrange(year, site)

site_year_df

```

The following code chunk will download the weather data for all unique combinations of year, site, and their coordinates in the "site_year_df" object.

```{r}

weather_daymet_all <- site_year_df %>% 
  mutate(weather = pmap(list(.y = year, 
                             .site = site, 
                             .lat = lat, 
                             .lon = lon), 
                        function(.y, .site, .lat, .lon) 
                          download_daymet( 
                            site = .site, 
                            lat = .lat, #specifying ".lat" placeholder for "lat = " argument
                            lon = .lon, #specifying ".lon" placeholder for "lon = " argument
                            start = .y, 
                            end = .y, 
                            simplify = T,
                            silent = T) %>% #end of " download_daymet()" function
                          rename(.year = year,
                                 .site = site) 
                        )) 


weather_daymet_all

```


The following code chunk will unnest the weather column. 

```{r}

weather_daymet_unnest <- weather_daymet_all %>%
  select(year, site, weather) %>% 
  unnest(weather) %>% 
  pivot_wider(names_from = measurement, 
              values_from = value) %>% 
  janitor::clean_names()

weather_daymet_unnest

View(weather_daymet_unnest)

```


# Merging the weather data retrieved from Daymet to the "merged_clean" data

The following code chunk will merge the weather data retrieved from Daymet to the "merged_clean" data

```{r merge_weather_with_data}

# Join daily weather onto your cleaned trial data by year & site

daymet_all_unnest <- merged_clean %>%
  left_join(
    weather_daymet_unnest,
    by = c("year", "site")
  )

fieldweather <- daymet_all_unnest

fieldweather

View(fieldweather)

```


## Exporting the merged data


```{r}

#write_csv(daymet_all_unnest,
          #"../data/merged_fieldweatherdata.csv"
          #)

```


# End of retreiving weather data from Daymet 

# Start of feature engineering

## Setup 

```{r}

#install.packages("ggridges")

library(ggridges)
library(tidyverse)

```



```{r}

#fieldweather <- read_csv("../data/merged_fieldweatherdata.csv")

#fieldweather

```

The following code chunk will keep desired variables that we will use further and get abbreviated month name based on the date.

```{r}

fe_month <- fieldweather %>%
  # Selecting needed variables
  dplyr::select(year, 
                site, 
                hybrid, 
                lon, 
                lat, 
                yday, 
                yield = adjusted_yield, 
                prev_crop,
                plant_doy,
                harvest_doy,
                soil.ph = soilpH, 
                soil.om.pct = om_pct, 
                soil.k.ppm = soilk_ppm, 
                soil.p.ppm = soilp_ppm, 
                dayl.s = dayl_s, #to rename variable name from "dayl_s" to dayl.s
                prcp.mm = prcp_mm_day, #to rename variable name to "prcp.mm"
                srad.wm2 = srad_w_m_2,#to rename variable name to "srad.wm2"
                tmax.c = tmax_deg_c, #to rename variable name to "tmax.c"
                tmin.c = tmin_deg_c,#to rename variable name to "tmin.c"
                vp.pa = vp_pa #to rename variable name to "vp.pa"
                ) %>%
  # Creating a date class variable  
  mutate(date_chr = paste0(year, "/", yday)) %>% 
  mutate(date = as.Date(date_chr, "%Y/%j")) %>% 
  # Extracting month from date  
  mutate(month = month(date)) %>% 
  mutate(month_abb = month(date, label = T)) #To get abbreviated month name e.g., Jan, Feb, Mar,...,Dec #month_abb is in "ord" (ordinal) format

fe_month
```



The following code chunk will feature engineer planting and harvest stages based on the following cut-offs:

Planting
•	early: DOY ≤ 120
•	normal: 121–150
•	late: > 150

Harvest
•	early: DOY ≤ 275
•	normal: 276–305
•	late: > 305


```{r}

fe_month <- fe_month%>%
  mutate(
    planting_stages = case_when(
      plant_doy <= 120                     ~ "early",
      plant_doy >= 121 & plant_doy <= 150  ~ "normal",
      plant_doy > 150                      ~ "late"
    ) %>% 
      fct_relevel("early","normal","late"),
    
    harvest_stages = case_when(
      harvest_doy <= 275                    ~ "early",
      harvest_doy >= 276 & harvest_doy <= 305 ~ "normal",
      harvest_doy > 305                     ~ "late"
    ) %>% 
      fct_relevel("early","normal","late")
  ) %>%
  select(-plant_doy, -plant_doy)

fe_month


```






The following code chunk will summarize daily weather variables based on month.  

```{r fe_month_sum}

fe_month_sum <- fe_month %>%
  group_by(year, site, hybrid, lon, lat, month_abb, yield, prev_crop, planting_stages, harvest_stages) %>% #If we do a summarise() after group_by(), any column that's not in the group_by() is gone, so we need to include "yield" in the group_by() to include it in the data frame because "yield" is our response variable so we must keep it in the data frame 
  #Because we are gonna be applying a "summarize()" function to different columns, we are gonna use a function called "across()" 
  summarise(across(.cols = c(soil.ph, 
                             soil.om.pct, 
                             soil.k.ppm, 
                             soil.p.ppm,
                             dayl.s,
                             srad.wm2,
                             tmax.c,
                             tmin.c,
                             vp.pa),
                   .fns = mean, #do not indicate the actual function "mean()", just use the word "mean"
                   .names = "mean_{.col}"), #specifying the weather variables that we want their mean as new column variables #1st across() is applying the "mean" function (to summarize "mean")
            across(.cols = prcp.mm,
                   .fns = sum,
                   .names = "sum_{.col}"
                   ) #specifying the weather variable (prcp.mm) that we want its sum as new column variable #2nd across() is summarizing sum #2nd across() is applying the "sum" function (to summarize "sum")
            ) %>%
  ungroup() #To convert from "group" to "tibble"


fe_month_sum

```


The following code chunk will check "tmax.c" and "prcp.mm" for the first site-year and month for the purpose of double checking to make sure that we did everything okay the way we intended. 

```{r}

fe_month %>%
  filter(year == 2014 & 
           site == "DEH1" &
           hybrid == "B37/MO17" &
           month_abb == "Jan") %>%
  summarise(tmax.c = mean(tmax.c),
            prcp.mm = sum(prcp.mm))


```

The code below will put the month as part of the column name. 

```{r fe_month_sum_wide}

fe_month_sum_wide <- fe_month_sum %>%
  pivot_longer(cols = mean_soil.ph:sum_prcp.mm) %>% 
  mutate(varname = paste0(name, "_", month_abb)) %>% 
  dplyr::select(-name, -month_abb) %>% 
  pivot_wider(names_from = varname,
              values_from = value) %>%
  # Rounding to one decimal point
  mutate(across(10:ncol(.), ~round(., 1) )) %>%
  select(-lon, -lat, -mean_soil.ph_NA, -mean_soil.om.pct_NA, -mean_soil.k.ppm_NA, -mean_soil.p.ppm_NA, -mean_dayl.s_NA, -mean_srad.wm2_NA, -mean_tmax.c_NA, -mean_tmin.c_NA, -mean_vp.pa_NA, -sum_prcp.mm_NA)


fe_month_sum_wide  

View(fe_month_sum_wide)

```


# EDA round 2  
Let's make a ridge plot to visualize the distribution of one variable over months.  

```{r, message=FALSE, warning=FALSE}

#install.packages("ggridges")
library(ggridges) #really powerful package to plot distributions e.g., density plot

ggplot(data = fe_month_sum,
       aes(x = mean_tmax.c,
           y = month_abb,
           fill = stat(x) #fill = stat(x): specific to "ggridges"
           )
       ) +
  geom_density_ridges_gradient(scale = 3,
                               rel_min_height = 0.01) + #"the 2nd argument is"rel_min_height = 0.01": to cut down the tails of the distribution to look nice
  scale_fill_viridis_c(option = "C") + #"viridis"scale_fill_viridis_c()": color-blind-friendly color scale #argument (option = "C") to change the color to "magma" because magma is weather related; "option = " varies from A to F
  theme(legend.position = "none") #"theme(legend.position = "none")": to remove the legend from the graph

```

Now let's do that for all weather variables [ we will automate using map2() to conduct iteration ]

```{r}

finalplots <- fe_month_sum %>%
  pivot_longer(cols = mean_soil.ph:sum_prcp.mm) %>% #we need to do pivot_longer() to iterate over the weather variables
  group_by(name) %>%
  nest() %>% #for iteration, we use "group_by()" followed by "nest()" in combo
  #we will use map2() since we need to iterate over 2 columns
  mutate(plot = map2(data, name, #map2() takes 2 arguments: the 1st argument becomes .x, the 2nd argument becomes .y #we must use map2() with a mutate() at first for iteration #"map2(data, name" : we want to iterate over "name" for "data"
                     ~ ggplot( data = .x, # .x represent "data" in the map2() function, so we need to use data = .x [= "data" from map2() ] to feed the "data" from map2() function as the data of ggplot() #.x is the iterating column of map2() that is a place holder for the 1st argument of map2()
       aes(x = value,
           y = month_abb,
           fill = stat(x)
           )
       ) +
  geom_density_ridges_gradient(scale = 3,
                               rel_min_height = 0.01) + 
  scale_fill_viridis_c(option = "C") + 
  theme(legend.position = "none") +
  labs(x = .y) #to rename the x-axis as the variable name # .y is the placeholder for "name" (i.e., variable name) in map2() function
                     )) 
  
finalplots

```




```{r}

finalplots$plot #$plot to print all the ggplots for each variables

```


```{r}

#write_csv(fe_month_sum_wide,
          # "../data/weather_monthsum.csv")

```


# End of feature engineering

# Start of XGboost

# ML Workflow with XGBoost

## Step 1: Load Libraries

We begin by loading the required libraries:

```{r}
#| message: false
#| warning: false
#install.packages("xgboost") #new pacakage
#install.packages("caret")

library(tidymodels)   # Core framework for modeling (includes recipes, workflows, parsnip, etc.)
library(finetune)     # Additional tuning strategies (e.g., racing, ANOVA-based tuning)
library(vip)          # For plotting variable importance from fitted models
library(xgboost)      # XGBoost implementation in R
library(ranger)       # Fast implementation of Random Forests
library(tidyverse)    # Data wrangling and visualization
library(doParallel)   # For parallel computing (useful during resampling/tuning)
library(caret)       # Other great library for Machine Learning 
```

The following code chunk will rename the "fe_month_sum_wide" data frame as "weather".   

```{r}

weather <- fe_month_sum_wide
weather

```

# ML workflow

## 1. Pre-processing

```{r weather_split}

set.seed(931735) # Setting seed to get reproducible results 

weather_split <- initial_split(
  weather, 
  prop = .7, # proption of split same as previous codes
  strata = yield  # Stratify by target variable
  )

weather_split

```

### a. Data split

For data split, let's use **70% training / 30% testing**.

```{r weather_train}

weather_train <- training(weather_split)  # 70% of data

weather_train #This is your traing data frame

```


```{r weather_test}

weather_test <- testing(weather_split)    # 30% of data

weather_test

```


### b. Distribution of target variable

```{r distribution}
ggplot() +
  geom_density(data = weather_train, 
               aes(x = yield),
               color = "red") +
  geom_density(data = weather_test, 
               aes(x = yield),
               color = "blue") 
  
```

### c. Data processing with recipe


Creating a recipe is an easy way to port your processing steps for other data sets without needing to repeat code, and also only considering the data it is being applied to.

You can find all available recipe step options here: https://tidymodels.github.io/recipes/reference/index.html

Different model types require different processing steps.\
Let's check what steps are required for an elastic net model (linear_reg). We can search for that in this link: https://www.tmwr.org/pre-proc-table #Don't need for numeric data


```{r weather_recipe}

# Create recipe for data preprocessing
weather_recipe <- recipe(yield ~ ., data = weather_train) %>% 
  # Remove identifier columns and months not in growing season
  step_rm(
    year,       # Remove year identifier
    site,       # Remove site identifier
    hybrid,     # Remove site identifier
    matches("Jan|Feb|Mar|Dec")  # Remove non-growing season months
  ) %>%
  # Turning every factor (prev_crop, planting_stages, harvest_stages, etc.) into dummies
  step_dummy(all_nominal_predictors())
  

weather_recipe

```

```{r weather_prep}
# Prep the recipe to estimate any required statistics
weather_prep <- weather_recipe %>% 
  prep()

# Examine preprocessing steps
weather_prep
```
## 2. Training

### a. Model specification

First, let's specify:\
- the **type of model** we want to train\
- which **engine** we want to use\
- which **mode** we want to use

> Elastic nets can only be run for a numerical response variable. XgBoost can be run with either numerical (regression) or categorical (classification) explanatory variable. Therefore, we have the need to specify the mode here.

XgBoost **hyperparameters**:

-   **`trees`**: The number of boosting rounds (i.e., how many trees will be added sequentially).\
-   **`tree_depth`**: Controls how deep each individual tree can grow. Deeper trees can capture more complex interactions but also increase the risk of overfitting.\
-   **`min_n`**: Minimum number of observations required in a node for it to be split. Acts as a regularization tool to prevent overly specific splits.\
-   **`learn_rate`**: Also known as `eta`, it controls how much each additional tree contributes to the overall model. Smaller values make the model more stable but require more trees.

```{r xgb_spec}

xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune() # ,  # Maximum depth of each tree
  #min_n = tune(),  # Minimum samples required to split a node
  #learn_rate = tune()
  ) %>% #Specifying XgBoost as our model type, asking to tune the hyperparameters
  set_engine("xgboost") %>% #specify engine 
  set_mode("regression")  # Set to mode
      
xgb_spec

```

### b. Cross-validation setup

We use 5-fold cross-validation to evaluate model performance during tuning:

```{r}

set.seed(235) #34549

resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5)

resampling_foldcv

resampling_foldcv$splits[[1]]

```
### c. Hyperparameter grid with Latin Hypercube Sampling

We use Latin hypercube sampling to generate a diverse grid of hyperparameter combinations:

```{r }

xgb_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  #min_n(),
  #learn_rate(),
  size = 10
)

xgb_grid
```

Note: the following code chunk would not work if "tree_depth" and "learn_rate" were not fine tuned in the "xgb_spec" object.

```{r}
ggplot(data = xgb_grid,
       aes(x = tree_depth, 
           y = min_n)) +
  geom_point(aes(color = factor(learn_rate), #coloring the bubbles based on learn_rate
                 size = trees), #size of the bubbles are based on the tress
             alpha = .5,
             show.legend = FALSE)
```

## 3. Model Tuning

```{r xgb_grid_result}

#install.packages("doParallel")
#install.packages("parallel")

library(doParallel)
library(parallel)

set.seed(76544)

#parallel processing
registerDoParallel(cores = parallel::detectCores()-1) #starts parallel processing

xgb_res <- tune_race_anova(object = xgb_spec,
                      preprocessor = weather_recipe,
                      resamples = resampling_foldcv,
                      grid = xgb_grid,
                      control = control_race(save_pred = TRUE))

stopImplicitCluster() #ends parallel processing

beepr::beep()

xgb_res$.metrics[[2]]

```


## 4. Select Best Models

We select the best models using three strategies (best, within 1 SE, within 2% loss) which we learned in class:

```{r}
# Based on lowest RMSE
best_rmse <- xgb_res %>% 
  select_best(metric = "rmse")%>% 
  mutate(source = "best_rmse")

best_rmse

```



```{r}
# Based on lowers RMSE within 1% loss
best_rmse_pct_loss <- xgb_res %>% 
  select_by_pct_loss("min_n",
                     metric = "rmse",
                     limit = 1
                     )%>% 
  mutate(source = "best_rmse_pct_loss")

best_rmse_pct_loss
```



```{r}
# Based on lowest RMSE within 1 se
best_rmse_one_std_err <- xgb_res %>% 
  select_by_one_std_err(metric = "rmse",
                        eval_time = 100,
                        trees
                        )%>% 
  mutate(source = "best_rmse_one_std_err")

best_rmse_one_std_err
```


Here we use all three methods which we learn in this class for R2.


```{r}
# Based on greatest R2
best_r2 <- xgb_res %>% 
  select_best(metric = "rsq")%>% 
  mutate(source = "best_r2")

best_r2
```



```{r}
# Based on lowers R2 within 1% loss
best_r2_pct_loss <- xgb_res %>% 
  select_by_pct_loss("min_n",
                     metric = "rsq",
                     limit = 1
                     ) %>% 
  mutate(source = "best_r2_pct_loss")

best_r2_pct_loss
```




```{r}
# Based on lowest R2 within 1 se
best_r2_one_std_error <- xgb_res %>% 
  select_by_one_std_err(metric = "rsq",
                        eval_time = 100,
                        trees
                        ) %>%
  mutate(source = "best_r2_one_std_error")

best_r2_one_std_error
```

## Compare and Finalize Model

```{r comparing values}
best_rmse %>% 
  bind_rows(best_rmse_pct_loss, 
            best_rmse_one_std_err, 
            best_r2, 
            best_r2_pct_loss, 
            best_r2_one_std_error)
```


## 5. Final Specification

```{r final_spec_fit}
final_spec <- boost_tree(
  trees = best_r2$trees,           # Number of boosting rounds (trees)
  tree_depth = best_r2$tree_depth #, # Maximum depth of each tree
  #min_n = best_r2$min_n,           # Minimum number of samples to split a node
  #learn_rate = best_r2$learn_rate  # Learning rate (step size shrinkage)
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

final_spec
```



## 6. Final Fit and Predictions

## Validation

```{r final_fit}
set.seed(10)
final_fit <- last_fit(final_spec,
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```


## 7. Evaluate on *Test Set*

```{r final_fit_metrics}
final_fit %>%
  collect_metrics()
```

 

## 8. Evaluate on Training Set

```{r}
final_spec %>%
  fit(yield ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(yield, .pred) %>%
  bind_rows(
    
    
# R2
final_spec %>%
  fit(yield ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rsq(yield, .pred))
```


## 9. Predicted vs Observed Plot

```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = yield,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous() +
  scale_y_continuous() 
```


## 10. Variable Importance

```{r final_spec}
final_spec %>%
  fit(yield ~ .,
         data = bake(weather_prep, weather_train)) %>% #There little change in variable improtance if you use full dataset
    vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```


# End of XGboost

# Start of Random Forest

# Setup  

```{r}
#| message: false
#| warning: false

#install.packages("ranger") #one of the packages in R that models random forest

library(tidymodels)
library(tidyverse) #takes a while to load
library(vip)
library(ranger)
library(finetune)
```

## 2. Random Forest Training 

### a. Random Forest Model specification  

First, let's specify:  
  - the **type of model** we want to train  
  - which **engine** we want to use  
  - which **mode** we want to use  


Random forest **hyperparameters**:  
  - **trees**: number of trees in the forest    
  - **mtry**: number of random features [or columns] sampled at each node split    
    - **min_n**: minimum number of data points in a node that are required for the node to be split further  
    
[Important note: we are not gonna fine tune "min_n" in this exercise because of time constraint, but we should fine tune "min_n" to do random forest in our own data]

Let's create a model specification that will **fine-tune** the first two for us.

A given model type can be fit with different engines (e.g., through different packages). Here, we'll use the **ranger** engine/package.  
  
```{r rf_spec}

rf_spec <- 
  # Specifying random forest as our model type, asking to tune the hyperparameters
  rand_forest(trees = tune(),
              mtry = tune() #both "trees" and "mtry" are model-type level hyperparameters, meaning that we fine tune "trees" and "mtry" inside "rand_forest()"
              ) %>%
    # Specify the engine (= package)
    set_engine("ranger") %>% #specifying "ranger" as the engine/package to run random forest 
    # Specifying mode  
    set_mode("regression") #random forest can handle both regression (when y is numerical) and classification (when y is categorical) #Here, we are specifying "set_mode("regression")" because our y variable is numerical [continuous]

rf_spec

```



### b. Hyper-parameter tuning  

For our iterative search, we need:  
  - Our model specification (`rf_spec`)  
  - The recipe (`weather_recipe`)  
  - Our **resampling strategy** (don't have yet) 
  

Let's define our resampling strategy below, using a 5-fold cross validation approach:  

```{r resampling_foldcv}

set.seed(34549)

rf_resampling_foldcv <- vfold_cv(weather_train, 
                              v = 5) #but at least 10 folds are recommended

rf_resampling_foldcv
rf_resampling_foldcv$splits[[1]]
rf_resampling_foldcv$splits[[2]]

```



We will use an iterative search algorithm called **simulated annealing**.  

In the algorithm below, we are asking for 10 iterations.  

```{r rf_grid_result}

set.seed(76544)

rf_grid_result <- tune_sim_anneal(object = rf_spec,
                     preprocessor = weather_recipe,
                     resamples = rf_resampling_foldcv,
                    #param_info = rf_param,
                    iter = 10 
                     )

beepr::beep()

rf_grid_result
rf_grid_result$.metrics[[2]]
```


Let's collect a summary of metrics (across all folds, for each iteration), and plot them.  

First, RMSE (lower is better):

```{r RMSE}
rf_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(x = mtry, 
             y = trees 
             )) +
  geom_path(group = 1) +
  geom_point(aes(color = mean),
             size = 3) + 
  scale_color_viridis_b() +
  geom_text(aes(label = .iter), nudge_x = .0005) +
  labs(title = "RMSE")
```


Now, let's look into R2 (higher is better):  

```{r R2}

rf_grid_result %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  ggplot(aes(x = mtry, 
             y = trees 
             )) +
  geom_path(group = 1) +
  geom_point(aes(color = mean),
             size = 3) + 
  scale_color_viridis_b() +
  geom_text(aes(label = .iter), nudge_x = .0005) +
  labs(title = "R2")

```


We want to see higher R2 (R2: higher is better).


The following code chunk will select the best model based on "percent loss" of the metric.

```{r}

# Based on lowest RMSE
best_rmse <- rf_grid_result %>%
  select_by_pct_loss("trees",
                     metric = "rmse",
                     limit = 2 #"limit = 2": this is how many percent losses you are accepting, 2% in this case
                     )

best_rmse

```


The following code chunk will select the best model based on greatest "R2"


```{r}

# Based on greatest R2
best_r2 <- rf_grid_result %>%
  select_by_pct_loss("trees",
                     metric = "rsq",
                     limit = 2
                     )


best_r2

```


I got the he following results that were different than Dr. Bastos's results (because of the randomness of 2 hyperparameters going on at each tree for a lot of models):

Based on RMSE, we would choose   
  - mtry = 25 
  - trees = 443

Based on R2, we would choose   
  - mtry = 22
  - trees = 113

Let's use the hyperparameter values that optimized R2 to fit our final model.

```{r final_spec}
final_spec <- rand_forest(trees = best_r2$trees,
                          mtry = best_r2$mtry) %>%
  # Specify the engine
  set_engine("ranger",
             importance = "permutation" #"permutation" is how we look into variable importance in random forest
             ) %>%
    # Specifying mode  
  set_mode("regression")
  

final_spec
```


## 3. Validation  

Now that we determined our best model, let's do our **last fit**.

### What does last fit mean? (Will come in exam)

This means 2 things:  
  - Training the optimum hyperparameter values on the **entire training set**  
  - Using it to **predict** on the **test set**  

These 2 steps can be completed in one function, as below:  

```{r final_fit}
final_fit <- last_fit(final_spec, 
                weather_recipe,
                split = weather_split)

final_fit %>%
  collect_predictions()
```


Metrics on the **test set**:

```{r}
final_fit %>%
  collect_metrics()
```

From the output,
Fit metrics on the test set:
rmse = 2.9317445	
rsq	= 0.1882245	

Metrics on **train set** (for curiosity and compare to test set):  
```{r}
# RMSE
final_spec %>%
  fit(yield ~ .,
      data = bake(weather_prep, 
                  weather_train)) %>%
  augment(new_data = bake(weather_prep, 
                          weather_train)) %>% 
  rmse(yield, .pred) %>%
  bind_rows(
    
    
    # R2
    final_spec %>%
      fit(yield ~ .,
          data = bake(weather_prep, 
                      weather_train)) %>%
      augment(new_data = bake(weather_prep, 
                              weather_train)) %>% 
      rsq(yield, .pred)
    
  )

```

From the output,
Fit metrics on the train set:
rmse = 1.1385078	
rsq	= 0.9470296	

How does metrics on test compare to metrics on train?  

Fit metrics on the test set:
rmse = 2.9317445	
rsq	= 0.1882245	

Fit metrics on the train set:
rmse = 1.1385078	
rsq	= 0.9470296	

We see that the R2 is really r-e-a-l-l-y high in the train set (rsq	= 0.9470296), as compared to the R2 of the test set 0.1882245. The RMSE is lower in the train set (rmse = 1.1385078) as compared to the RMSE of the test set (rmse = 2.9317445). [This is the reason why it's okay to report the fit metrics of the train set to understand how well our model is explaining our data, but it is not a good measure of the predictive ability of the model. To understand the predictive ability of the model, we have to look at the fit metrics of the test set. Notice that the R2 of the test set was 0.1882245, whereas the R2 of the train set was 0.9470296 -- which is a very large difference. Hence, *we need to always report the fit metrics of the test set* along with the fit metrics of the train set. ]

Predicted vs. observed plot:  
```{r}
final_fit %>%
  collect_predictions() %>%
  ggplot(aes(x = yield,
             y = .pred)) +
  geom_point() +
  geom_abline() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(20, 40)) +
  scale_y_continuous(limits = c(20, 40)) 
```


[**Very important note:**
To interpret random forest model, we should always expcet to see (1) *perdicted vs. observed plot* and (3) *variable importance plot*, at the least -- these are the 2 things that must be mentioned in a random forest model. 
]

*Variable importance:* 

[For Random Forest,] The importance metric we are evaluating here is **permutation**. 

In the permutation-based approach, for each tree, the out-of-bag sample is passed down the tree and the prediction accuracy is recorded. [as is]   

Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed.   

The decrease in accuracy as a result of this randomly shuffling of feature values is averaged over all the trees for each predictor.   

The variables with the **largest average decrease in accuracy** [after being permuted] are [the ones that are] considered **most important**.  

```{r}

final_spec %>%
  fit(yield ~ .,
         data = bake(weather_prep, weather)) %>%
    vi() %>%
  mutate(
    Variable = fct_reorder(Variable, 
                           Importance)
  ) %>%
  ggplot(aes(x = Importance, 
             y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
    
```


#End of Random Forest

#Start of getting yield prediction (yield_mg_ha) for the unseen data in "testing_submission.csv" file


